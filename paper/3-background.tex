\section{Background}
\label{sec:background}

We begin with a brief background on relevant topics.

\subsection{Lambdas/Serverless Functions}
\label{sec:background:lambdas}
\todo{needs sprucing}
One of the fast-growing cloud services in recent years are serverless functions such as lambdas on AWS~\cite{awslambda} and cloud functions on GCP~\cite{gcpfunctions}. This interest stems from the fact that serverless architecture does not require the developer to worry about provisioning, maintaining, and administering servers. Additionally, lambdas are much more cost-efficient than VMs as they allow more efficient packing of the servers. However, these functions are more ephemeral than containers and VMs, in many cases only  few minutes. While this attribute provides more flexibility in cost and functionality, the nature of serverless functions also increases the difficulty in detecting co-residency and launching successful attacks.

We focus on AWS Lambdas in this paper but we show that our study is applicable to other clouds as well. Lambdas execute as much smaller units than containers and virtual machines. Lambda sizes range from 128 MB to 3 GB, and their maximum timeout value is 15 minutes. While lambdas are limited in the computations they can execute, they are conversely incredibly lightweight and can be initiated and deleted in a very short amount of time. Since lambdas are short-lived and lightweight, the user has no control over the physical location of the server(s) on which their lambdas are spawned. 

%This seems like a repeat of similar content in other places in the paper, so I've commented it out for now - AM
%In addition to being a relatively recent addition to the long list of %cloud computing services provided, the seemingly fleeting nature of %Lambdas make it challenging to perform targeted co-location attacks %using hardware channels. This could potentially be the reason why %Lambdas have not been explored as much as virtual machines and %containers in literature. But the same characteristics of Lambdas are %what make them interesting to explore, which is what we set out to do %in this work.

\subsection{Co-residence Detection}
\label{sec:background:pastwork}

%On previous work on colocation, past techniques and why those techniques would 
%not work anymore
To perform side-channel attacks against other tenants in a cloud setting, attackers 
need to co-locate their applications on the same servers as their victims. Past 
research has used various strategies to achieve co-residency for demonstrating 
such attacks. Typically, achieving co-residency includes a (VM/Container) launch 
strategy (varying number of instances, time of the day, etc) combined with a 
co-residence detection mechanism 
for detecting if two instances are running on the same machine. Traditionally, 
this was done based on software runtime information like public/internal IP addresses\cite{ristenpartccs2009}, files in \textit{procfs} or other environment variables\cite{wangusenix2018} and other such logical side-channels\cite{varad191016,vmplacement}
that two instances running on a same server might share. 

As virtualization platforms move towards stronger isolation between instances 
(e.g. AWS' Firecracker VM \cite{firecracker}), these logical side-channels have 
become less effective or infeasible. Furthermore, some of these side-channels were only 
effective on container-based platforms that share the underlying OS image and were 
less suitable for hypervisor-based platforms. This prompted a move 
towards hardware-based covert channels which can bypass software isolation and are usually harder to fix. Typically, these covert channels involve 
sending/receiving information by causing contention on a shared hardware that 
results in observable performance fluctuations across applications. A 
number of such side-/covert channels based on shared hardware like last-level caches \todo{references}, memory bus~\cite{wuusenix2012,zhang2016,varadarajan2015} and 
storage devices\todo{references} have been explored in the past, some 
of which have already been addressed and are no longer even feasible in 
most clouds (e.g., last-level cache-based channels\cite{cache-sidechannels}). 

%  \subsection{Random Number Generator hardware}
%  \label{sec:background:rng}
%  When examining avenues for co-location on lambdas, one avenue we explored is the random number generator (RNG) hardware. Modern processors support a shared hardware module to generate true random numbers. These devices use low level noise signals such as thermal noise and other quantum phenomena to produce true non deterministic entropy. Information from this module is routed from the host machine to the /dev/random file in the guest virtual machine for cryptographic operations. Since the hardware is shared, if one guest consumes these random bits within an infinite loop, another user could notice a spike in random operations, indiciating contention.

% Rdrand and rdseed are the two instructions used to access random bits produced by the RNG generator hardware. The bits in the conditional buffer are directly used by the rdseed instruction, but rdrand uses another deterministic module that depends on the past outputs, which helps rdrand achieve a higher throughput than rdseed. This design makes rdseed more reliable to use as rdseed instructions are easily and quickly exhaustible. 

\subsection{Memory Bus Covert Channel}
\label{sec:background:membus}
One shared hardware that we examine in our study is the memory bus. The memory bus is the piece of hardware that connects the memory controller to main memory. Memory bus contention can be caused by initiating repeated memory accesses to saturate the bus bandwidth and cause observable latency spikes. However, this turns out to be very challenging given the multiple levels of caches on today's servers, which prevent repeated memory accesses, and the high memory bandwidth that cannot be saturated by few CPU cores. 

In x86 systems, atomic memory instructions designed to facilitate multi-processor synchronization are supported by cache coherence protocols as long as the operands stay within a cache line (which is generally the case as language compilers make sure that operands are aligned). However, if the operand is spread across two cache lines (referred to as "exotic" memory operations), x86 hardware achieves atomicity by locking the memory bus to prevent any other memory access operations until the current operation finishes. This
 results in significantly higher latencies for the other operations
 which cannot use the ample memory bandwidth due to the lock\cite{wuusenix2012}. Furthermore, this behaviour persists even in the presence of multiple processor sockets, making the locking effects visible to all the cores on the machine. We exploit this property of x86 hardware to cause contention on the memory bus and use the 
 resulting observable variations in performance as a covert channel for detecting co-residency. 
