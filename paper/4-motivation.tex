% \section{Motivation}
% \label{sec:motivation}

% % Does adding a threat model section make sense?
% \amirian{Given the intro, I think we can safely remove this section. I've tried to work in pertinent parts into the intro}

% % but whatever works for lambdas is applicable to containers & VMs...
% % can we definitively say this?

\section{Methodology}
\label{sec:methodology}

% TODO: This figure shows only mem access latencies of exotic 
% operation. how does these operations affect latencies of other exotic 
% operations or regular memory accesses?
\begin{figure*}[h!]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_aws.pdf}
%   \caption{1a}
%   \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_azure.pdf}
%   \caption{1b}
%   \label{fig:sfig2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_gcp.pdf}
%   \caption{1b}
%   \label{fig:sfig2}
\end{subfigure}
\caption{From left to right, the plots show the latencies of atomic 
      memory operations performed on an 8B memory region as we slide it 
      from one cache line across the boundary into another, on AWS, Azure and 
      Google (GCP) clouds respectively. The latencies 
      are orders of magnitude higher when the 8B region falls across the 
      two cache lines (offsets 0-7B) demonstrating the presence of 
      the memory bus covert channel on all these clouds. \label{fig:membus_clouds}}
\label{fig:fig}
\end{figure*}

% reliable - do we want to say something about this?
\amirian{make sure that all these terms are defined ahead of time: co-location,
co-operative, co-residence, serverless, lambdas}

Our goal is to determine a co-operative co-residence detection mechanism for
serverless functions. In other words, given a series of spawned lambdas in a
given region on a cloud service, how can we determine the lambdas that are
co-located on the same machines?  In this section, we discuss the details of
such a mechanism, previous solutions to this problem, and the unique challenges
we faced with lambda co-residence.

Given a set of cloud instances (VMs, Containers, Functions, etc) deployed
to a public cloud, a co-residence detection mechanism would identify, for each 
pair of instances in the set, whether the pair was running on the same physical 
server at some point. Paraphrasing Varadarajan et al.\cite{varadarajan2015}, for 
a co-detection mechanism to be useful across a wide range of 
launch strategies, we observe that it should have the following desirable 
properties:

\begin{itemize}
    \item \textbf{Generic} The technique should be applicable across a wide
    range of server architectures and software runtimes. In practice, the
    technique would work across most third-party clouds and even among different
    platforms within a cloud.
    \item \textbf{Reliable} The technique should have a reasonable detection success
    with minimal false negatives (co-resident instances not detected) and even 
    less false positives (non co-resident instances categorized as co-resident).
    \item \textbf{Scalable} A launch strategy may require hundreds or even thousands 
    of instances to be deployed, and must be fast and scalable such that the 
    technique will take less time to detect all co-resided pairs at a reasonable cost.
\end{itemize}

Given these properties, we decide to investigate harware-based covert channels.
Hardware-based covert-channels are more difficult to remove and obfuscate than
software-based covert channels, and are also more ubiquitous, given that
hardware is more homogenous in nature than software. 
% generic

\subsubsection{RNG Hardware}
\amirian{This feels a bit awkward. We might want to move this to discussion}
We first examined covert channels based on Random Number Generator (RNG)
hardware\cite{evtyushkinccs2016}.  Modern processors support this shared
hardware module to generate true random numbers. 
% These devices use low level noise signals such as thermal noise and other
% quantum phenomena to produce true non deterministic entropy. 
To produce true random numbers, information from this module is routed from the
host machine to the /dev/random file in the guest virtual machine for
cryptographic operations. Since the hardware is shared, if one guest consumes
these random bits within an infinite\amirian{does it need to be infinite?} loop,
another user could notice a spike in random operations, indicating contention on
the machine.
\amirian{do we have an old graph to back up this point of being too noisy? to
just drive the point home that RNG is not great for these purposes} 
However, our experiments on AWS using RNG hardware indicated that this channel
is unreliable for lambda co-detection. We hypothesize that perhaps, because
causing contention is easy, the channel gets too noisy as a result to accurately
use for our purposes.

% In order to get random bits, we used a Python module called rdrand that supports 
% both rdrand and rdseed instructions. While our initial plan was to run only rdseed 
% instructions which seemed better suited for our attacker logic, we found that 
% AWS hosts did not support rdseed. We also observed that rdseed was not supported 
% by few of the hosts on GCP. Hence, we run a unified program that uses rdseed
% when available and falls back on rdrand otherwise.\todo{I think we ended up 
% ditching the rdseed idea and stuck to just rdrand. (confirm just in case)} 

% We ran a simple experiment to determine if RNG techniques would be fruitful 
% for our goals. In the first run, we labeled lambdas as victims or attackers. 
% Victim lambdas would take their first set of measurements, sleep for five seconds, 
% and then take a second set of measurements. Attacker lambdas would sleep for the 
% first six seconds (long enough to allow victims to sample once without possible contention), 
% and then start "attacking". In the second run, we simply had victim lambdas run without 
% any attackers causing possible contention.  We show our results in figure~\todo{add the figure in}, 
% where the red dots are the victim lambdas that executed with the presence of attacker 
% lambdas, and the blue dots are victim lambdas that executed alone. There does not appear 
% to be a significant difference between executions when an attacker does and does not exist, 
% which indicates that RNG hardware might not be a salient avenue for determining 
% lambda co-location, as we are not able to differentiate when contention is happening.

\subsubsection{Memory bus channel}
We next explored (and ultimately used) the memory bus covert channel described
in section~\ref{sec:background:membus} as it exploits a fundamental hardware
vulnerability that is present across all generations of x86 hardware.
Historically, multiple public cloud services have been vulnerable to this
channel~\cite{varad191016,compstudycoresidency}, and we found that they are
still vulnerable today. \amirian{I think there should be another sentence or two
describing how we got to this figure right here. It sort of feels like we just
jump into results}.
Moreover, we were able to demonstrate this behavior through serverless function
instances, whose runtimes are mostly restricted to high-level languages that
prevent the pointer arithmetic required to perform these exotic operations.  To
demonstrate this attack, we used the unsafe environments (C++ on AWS, Unsafe Go
on GCP, Unsafe C\# On Azure) that these clouds allowed. 
Figure~\ref{fig:membus_clouds} shows that all three major cloud providers still
exhibit significant difference in latencies for the "exotic" memory locking
operations when compared to regular memory access latencies. This figure shows
the applicability of using memory bus across different kinds of cloud instances
as well.


% why previous approaches were not scalable
\subsubsection{Previous approaches using Memory bus}
Previous works that used the memory bus for co-residence detection divide the
deployed instances into attack and (co-operative) victim roles, and attempt to
co-locate the attacker instances with a victim instance. The attack roles
continually lock the memory bus (locking process) for a certain duration
(\textasciitilde 10 seconds) while the victims sample the memory for any spike
in access latencies (probing process). If all the deployed instances try the
detection i.e., locking and probing at once, (some of) the victims may see
locking effects, but there would be no way of knowing which or how many attack
roles co-resided with a particular victim and caused the locking. This provides
no information about the number of physical servers that ran these instances or
the amount of co-location. The only information we can deduce is that victims
were probably co-located with just a single attacker.

An alternative method is to try pair-wise detection where only one attack
instance locks and one victim instance probes at a time revealing co-residence
of this pair, and repeating this serially for each pair. However, this technique
is too slow and scales quadratically with the number of instances e.g., a
hundred instances take more than 10 hours assuming 10 secs for each pair.
\todo{get a cost estimate on ec2}. Varadarajan et al.\cite{varad191016} speeds
this process significantly by performing detection for mutually-exclusive
subsets in parallel, allowing for false-positives and later eliminating the
false-positives sequentially.\amirian{might want to elaborate on this; on its
own may be a bit confusing} This would still only scale linearly in the best
case \todo{(or not even that?)}, which is still expensive - with a thousand
instances, for example, the whole detection process takes well over 2 hours to
finish, which is infeasible for lambdas that are, by nature, ephemeral. Thus,
one challenge in this work is creating a faster neighbor detection algorithm.

% how do we scale it - challenges.
\subsubsection{The Path to Scalability}

One method to quicken the co-location process is by cutting down on the time it
takes for single attack-victim pair to determine co-residence i.e., improving
upon probing time and accuracy of the victim. However, this only affects total
time by a constant factor. To improve scalability, we need to be able to run
detection for two\amirian{why two?} attack-victim pairs in parallel without
sacrificing the certainty when they are run serially. For example, when both
pairs see co-residence, we must be certain that each victim experienced
co-residence because of its own attacker pair, which is not possible if
co-residence is ascertained based a simple yes/no signal from the attack
instance. 

The memory bus convert channel was used to exchange more complex
information like keys in previous work\cite{wuusenix2012}, and at first sight,
can be used to exchange information such as IDs between the co-resided instances
to solve our problem.  However, the original work assumes that there is only one
sender and one receiver who know exactly how and when to communicate. As we will
see in the next section, this model is not sustainable when there exist many
parties that have no knowledge of each other but try to communicate on the same
channel.

To solve some of the challenges mentioned previously, we propose a protocol in
which we use the memory busy covert channel to exchange IDs between instances,
and the co-resided instances reliably exchange their IDs with each other to
discover their neighbors. \amirian{this sentence is a bit confusing}The protocol
takes time on the order of number of instances involved, which is limited by the
maximum number of co-located instances on a single server (tens) - something
that is orders of magnitude less than total number of instances deployed to the
cloud (hundreds to thousands).  This lets us scale our co-residence detection
significantly to run in less than a minute.\amirian{seconds? minutes?}.

% ofcourse, this only works if all the instances are in our control 
% so we limit our focus to cooperative co-residence detection - where do 
% we add this?
