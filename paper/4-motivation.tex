% \section{Motivation}
% \label{sec:motivation}

% % Does adding a threat model section make sense?
% \amirian{Given the intro, I think we can safely remove this section. I've tried to work in pertinent parts into the intro}

% % but whatever works for lambdas is applicable to containers & VMs...
% % can we definitively say this?

\section{Methodology}
\label{sec:methodology}

% TODO: This figure shows only mem access latencies of exotic 
% operation. how does these operations affect latencies of other exotic 
% operations or regular memory accesses?
\begin{figure*}[h!]
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_aws.pdf}
%   \caption{1a}
%   \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_azure.pdf}
%   \caption{1b}
%   \label{fig:sfig2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=.99\linewidth]{fig/membus_gcp.pdf}
%   \caption{1b}
%   \label{fig:sfig2}
\end{subfigure}
\caption{From left to right, the plots show the latencies of atomic 
      memory operations performed on an 8B memory region as we slide it 
      from one cache line across the boundary into another, on AWS, GCP and 
      Azure clouds respectively. \todo{Get plot for GCP}. The latencies 
      are orders of magnitude higher when the 8B region falls across the 
      two cache lines (offsets 0-7B) demonstrating the presence of 
      the memory bus covert channel on all these clouds. \label{fig:membus_clouds}}
\label{fig:fig}
\end{figure*}

% reliable - do we want to say something about this?
Our goal for this paper is to determine a co-operative co-residence detection mechanism for 
serverless functions. In other words, given a series of spawned lambdas in a given region 
on a cloud, how can we determine the lambdas that are co-located on the same machines?
In this section, we discuss what such mechanism must look like, previous solutions to 
this problem, the unique challenges we faced with lambdas and the choices we made. 

Given a set of cloud instances (VMs, Containers, Functions, etc) deployed
to a public cloud, a co-residence detection mechanism would identify, for each 
pair of instances in the set, whether the pair was running on the same physical 
server at some point. Paraphrasing Varadarajan et al.\cite{varadarajan2015}, for 
the mechanism to be useful across wide range of 
launch strategies, we observe that it should have the following desirable 
properties:

\begin{itemize}
    \item \textbf{Generic} The technique should be applicable across wide 
    range of server architectures and software runtimes. In practice, the technique 
    would work across most third-party clouds and 
    even among different platforms within a cloud.
    \item \textbf{Reliable} The technique should have a reasonable detection success
    with minimal false negatives (co-resident instances not detected) and even 
    less false positives (non-co-resident instances categorized as co-resident).
    \item \textbf{Scalable} A launch strategy may require hundreds or even thousands 
    of instances to be deployed, and must be fast and scalable such that the 
    technique will take less time to detect all co-resided pairs at a reasonable cost.
\end{itemize}

% generic

\subsubsection{RNG Hardware}
We opted for hardware-based covert channels as they are much more difficult to 
remove than software attributes, and are more likely to be widely applicable 
given that hardware is more homogeneous than software. We first examined 
covert channels based on Random Number Generator (RNG) hardware\cite{evtyushkinccs2016}.
Modern processors support this shared hardware module to generate true random numbers. 
% These devices use low level noise signals such as thermal noise and other quantum 
% phenomena to produce true non deterministic entropy. 
Information from this module is routed from the host machine to the /dev/random file 
in the guest virtual machine for cryptographic operations. Since the hardware is shared, 
if one guest consumes these random bits within an infinite loop, another user could 
notice a spike in random operations, indicating contention. However, our experiments on 
AWS on this hardware showed us that this channel is unreliable perhaps because
causing contention is easy and the channel gets too noisy as a result.

% In order to get random bits, we used a Python module called rdrand that supports 
% both rdrand and rdseed instructions. While our initial plan was to run only rdseed 
% instructions which seemed better suited for our attacker logic, we found that 
% AWS hosts did not support rdseed. We also observed that rdseed was not supported 
% by few of the hosts on GCP. Hence, we run a unified program that uses rdseed
% when available and falls back on rdrand otherwise.\todo{I think we ended up 
% ditching the rdseed idea and stuck to just rdrand. (confirm just in case)} 

% We ran a simple experiment to determine if RNG techniques would be fruitful 
% for our goals. In the first run, we labeled lambdas as victims or attackers. 
% Victim lambdas would take their first set of measurements, sleep for five seconds, 
% and then take a second set of measurements. Attacker lambdas would sleep for the 
% first six seconds (long enough to allow victims to sample once without possible contention), 
% and then start "attacking". In the second run, we simply had victim lambdas run without 
% any attackers causing possible contention.  We show our results in figure~\todo{add the figure in}, 
% where the red dots are the victim lambdas that executed with the presence of attacker 
% lambdas, and the blue dots are victim lambdas that executed alone. There does not appear 
% to be a significant difference between executions when an attacker does and does not exist, 
% which indicates that RNG hardware might not be a salient avenue for determining 
% lambda co-location, as we are not able to differentiate when contention is happening.

\subsubsection{Memory bus channel}
We next explored (and ultimately used) the memory bus covert channel 
described in section~\ref{sec:background:membus} as it exploits a fundamental hardware 
vulnerability that is present across all generations of x86 hardware. 
Multiple public clouds have been shown to be 
vulnerable  to this channel years ago~\cite{varad191016,compstudycoresidency}, 
and we found that they 
are still vulnerable now. Figure~\ref{fig:membus_clouds} shows that all three major cloud providers 
still exhibit significant difference in latencies for the "exotic" memory locking operations when
compared to regular memory access latencies. Moreover, we were able to demonstrate this behavior 
through serverless
function instances whose runtimes were mostly restricted to high-level languages that 
prevent the pointer arithmetic required to perform these exotic operations. 
To demonstrate this attack, we used 
the unsafe environments (C++ on AWS, Unsafe Go on GCP, Unsafe C\# On Azure) that these 
clouds allowed. \todo{elaborate} This shows the applicability of this covert 
channel across different kinds of cloud instances as well.

% why previous approaches were not scalable
\subsubsection{Previous approaches using Memory bus}
Previous works that used memory bus for co-residence 
detection divide the deployed instances into 
attack and (co-operative) victim roles, and attempt to 
co-locate the attacker instances with a victim instance. The attack roles
continually lock the memory bus (locking process) for a 
certain duration (\textasciitilde 10 seconds) while the victims sample the 
memory for any spike in access latencies (probing process). 
If all the deployed instances try the detection i.e., locking and probing
at once, (some of) the victims may see locking effects, but there would be no way of
knowing which or how many attack roles co-resided with a particular victim and 
caused the locking. This provides no information about the number of physical servers 
that ran these instances or the amount of co-location. The only information 
we can deduce is that victims were probably co-located with just a single attacker.

An alternative method is to 
try pair-wise detection where only one attack instance locks and one victim 
instance probes at a time revealing co-residence of this pair, 
and repeating this serially for each pair. However, this technique is too slow and 
scales quadratically with the number of instances e.g., a hundred instances take 
more than 10 hours assuming 10 secs for each pair. \todo{get a cost estimate 
on ec2}. Varadarajan et al.\cite{varad191016} speeds this process significantly by performing 
detection for mutually-exclusive subsets in parallel, allowing for false-positives 
and later eliminating the false-positives sequentially. \todo{elaborate?} 
This would still only scale linearly in the best case \todo{(or not even that?)}, 
which is still expensive - with a thousand instances, for example, 
the whole detection process takes well over 2 hours to finish, which is infeasible 
for lambdas that are, by nature, ephemeral. Thus, one challenge in this work is 
creating a faster neighbor detection algorithm.

% how do we scale it - challenges.
\subsubsection{The path to scalability}

One method to quicken the process is by cutting down on the time 
it takes for single attack-victim pair to determine co-residence
i.e., improving upon probing time and accuracy of the victim. However,
this only affects total time by a constant factor. To improve
scalability, we need to be able to run detection for two attack-victim 
pairs in parallel without sacrificing the information we get with certainty 
when they are run serially. For example, when both pairs see 
co-residence, we must be certain that each victim experienced co-residence because of its 
own pair, which is not possible if co-residence is ascertained based a simple yes/no signal from the attack 
instance.  Memory bus convert channel was used to exchange 
more complex information like keys in previous work\cite{wuusenix2012}, 
and at first sight, can be used to exchange information such as IDs between the 
co-resided instances to solve our problem. However, the original work assumes that 
there is only one sender and one receiver who know exactly how 
and when to communicate. As we will see in the next section, this model is not sustainable
when there exist many parties that have no knowledge of each other but try to communicate
on the same channel.

To side-step some of the challenges mentioned previously, we simplify the requirement on 
the covert channel to just exchange IDs and propose a protocol which the co-resided 
instances can use to reliably exchange their IDs with each other and discover their neighbors. 
The protocol takes time on the order of number of instances involved, which is limited by 
maximum number of co-located instances on a single server (tens) - something that is orders 
of magnitude less than total number of instances deployed to the cloud (hundreds to thousands). 
This lets us scale our co-residence detection significantly. 

% ofcourse, this only works if all the instances are in our control 
% so we limit our focus to cooperative co-residence detection - where do 
% we add this?